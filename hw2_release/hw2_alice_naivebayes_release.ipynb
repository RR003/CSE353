{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alice', 'adventures', 'in', 'wonderland', 'by', 'lewis', 'carroll', 'the', 'millennium', 'fulcrum', 'edition', '3', 'contents', 'chapter', 'i', 'down', 'the', 'rabbit', 'chapter', 'ii', 'the', 'pool', 'of', 'tears', 'chapter']\n",
      "corpus len:  25320\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "f = open('alice_in_wonderland.txt','r')\n",
    "while(1):\n",
    "    line =  f.readline()\n",
    "    if len(line) == 0: break\n",
    "    corpus.extend(line.split())\n",
    "        \n",
    "f.close()\n",
    "corpus = ' '.join(corpus)\n",
    "\n",
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "    for punctuation in ['\"',\"'\",'.',',','-','?','!',';',':','â€”','(',')','[',']']:\n",
    "        word = word.split(punctuation)[0]\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "corpus = [clean_word(word) for word in corpus.split()]\n",
    "corpus = [word for word in corpus if len(word) > 0]\n",
    "print(corpus[:25])\n",
    "D = len(corpus)\n",
    "print('corpus len: ',D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word list size (number of distinct words):  2501\n"
     ]
    }
   ],
   "source": [
    "tokenize = {}\n",
    "wordlist = []\n",
    "token = 0\n",
    "for word in corpus:\n",
    "    if word not in tokenize.keys():\n",
    "        tokenize[word] = token\n",
    "        wordlist.append(word)\n",
    "        token += 1\n",
    "    \n",
    "V = len(wordlist)\n",
    "print('word list size (number of distinct words): ', V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [9. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# bin how many times a word follows another word\n",
    "counts_2gram = np.zeros((V,V))\n",
    "for i in range(1,len(corpus)):\n",
    "    token_i = tokenize[corpus[i]]\n",
    "    token_im1 = tokenize[corpus[i-1]]\n",
    "    counts_2gram[token_i,token_im1] += 1\n",
    "print(counts_2gram)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\n",
      "0.005374061880700568\n",
      "said\n",
      "0.0023591151671500023\n",
      "the\n",
      "0.03960014516886554\n",
      "mock\n",
      "0.0021748734838139606\n"
     ]
    }
   ],
   "source": [
    "#past word as feature\n",
    "\n",
    "posterior_1word = np.zeros((V, V))\n",
    "prior = np.zeros(V)\n",
    "\n",
    "def fill_prior():\n",
    "    map = {}\n",
    "    for word in corpus:\n",
    "        if word in map:\n",
    "            map[word] += 1\n",
    "        else:\n",
    "            map[word] = 1\n",
    "    \n",
    "    index = 0     \n",
    "    for word in wordlist:\n",
    "        token = tokenize[word]\n",
    "        prior[token] = map[word]/len(corpus)\n",
    "        \n",
    "\n",
    "\n",
    "def fill_in_posterior_1word():\n",
    "    for word in wordlist:\n",
    "        token = tokenize[word]\n",
    "        wordArray = counts_2gram[token]\n",
    "            \n",
    "        total = 0\n",
    "        for count in wordArray:\n",
    "            total += count\n",
    "                \n",
    "        newArray = []\n",
    "            \n",
    "        for count in wordArray:\n",
    "            newArray.append(count / total)\n",
    "            \n",
    "        posterior_1word[token] = newArray  \n",
    "        \n",
    "fill_in_posterior_1word()\n",
    "fill_prior()\n",
    "    \n",
    "def pred_2gram(word):\n",
    "    token = tokenize[word]\n",
    "    \n",
    "    array = posterior_1word[token]\n",
    "    \n",
    "    index = 0\n",
    "    value = 0\n",
    "    for i in range(len(array)):\n",
    "        if array[i] > value:\n",
    "            value = array[i]\n",
    "            index = i\n",
    "    ## print(value)\n",
    "    \n",
    "    print(wordlist[index])\n",
    "    return posterior_1word[token][index] * prior[index]  \n",
    "    \n",
    "print(pred_2gram('alice'))\n",
    "print(pred_2gram('the'))\n",
    "print(pred_2gram('cat'))\n",
    "print(pred_2gram('turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24535903309898097\n"
     ]
    }
   ],
   "source": [
    "## posterior_1word = np.zeros((V, V))\n",
    "\n",
    "def whole_corpus_2gram():\n",
    "    index = 0\n",
    "    count = 0\n",
    "    for word in corpus:\n",
    "        prevToken = tokenize[word]\n",
    "        \n",
    "        if index >= len(corpus) - 1:\n",
    "            break\n",
    "        \n",
    "        newToken = 0\n",
    "        value = -1\n",
    "        for i in range(V):\n",
    "            temp = posterior_1word[i][prevToken]\n",
    "            temp *= prior[i]\n",
    "            if temp > value:\n",
    "                value = temp\n",
    "                newToken = i\n",
    "                \n",
    "        \n",
    "            \n",
    "        newWord = wordlist[newToken]\n",
    "        if newWord == corpus[index + 1]:\n",
    "            count += 1\n",
    "        \n",
    "        index += 1\n",
    "    return count / (index - 1)\n",
    "\n",
    "print(whole_corpus_2gram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_posterior(size):\n",
    "    posterior_word = np.zeros((size,V, V))\n",
    "    \n",
    "    for k in range(size):\n",
    "        counts_gram = np.zeros((V,V))\n",
    "        for i in range(k+1,len(corpus)):\n",
    "            token_i = tokenize[corpus[i]]\n",
    "            \n",
    "            \n",
    "            for j in range(i-k-1, i):\n",
    "                token_im1 = tokenize[corpus[j]]\n",
    "                counts_gram[token_i,token_im1] += 1\n",
    "        \n",
    "                \n",
    "        posterior_word_temp = np.zeros((V,V))\n",
    "        \n",
    "        for word in wordlist:\n",
    "            token = tokenize[word]\n",
    "            wordArray = counts_gram[token]\n",
    "            \n",
    "            total = 0\n",
    "            for count in wordArray:\n",
    "                total += count\n",
    "            newArray = []\n",
    "            for count in wordArray:\n",
    "                if total == 0:\n",
    "                    newArray.append(0)\n",
    "                else:\n",
    "                    newArray.append(count / total)\n",
    "\n",
    "            posterior_word_temp[token] = newArray \n",
    "            \n",
    "        posterior_word[k] = posterior_word_temp\n",
    "    return posterior_word\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#past k words as features\n",
    "posterior_word_copy = create_posterior(4)\n",
    "def pred_word(array):\n",
    "    \n",
    "    main_array = np.ones(V)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        posterior_word_temp = posterior_word_copy[i]\n",
    "        word = array[len(array) - 1 - i]\n",
    "        token = tokenize[word]\n",
    "         \n",
    "        for j in range(V):\n",
    "            main_array[j] *= posterior_word_temp[j][token]\n",
    "           \n",
    "    \n",
    "    finalToken = 0\n",
    "    value = -1\n",
    "    for i in range(len(main_array)):\n",
    "        if (main_array[i] * prior[i] > value):\n",
    "            \n",
    "            value = main_array[i] * prior[finalToken]\n",
    "            finalToken = i\n",
    "    ## print(value)\n",
    "    return wordlist[finalToken]\n",
    "\n",
    "def pred_word_list_array(array):\n",
    "    \n",
    "    main_array = np.ones(V)\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        posterior_word_temp = posterior_word_copy[i]\n",
    "        word = array[len(array) - 1 - i]\n",
    "        token = tokenize[word]\n",
    "         \n",
    "        for j in range(V):\n",
    "            main_array[j] *= posterior_word_temp[j][token]\n",
    "           \n",
    "    \n",
    "    finalToken = 0\n",
    "    value = -1\n",
    "    for i in range(len(main_array)):\n",
    "        main_array[i] = main_array[i] * prior[i]\n",
    "\n",
    "    return main_array\n",
    "    \n",
    "    \n",
    "## print(pred_word(['what','an','ignorant','little']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47337862390394186\n"
     ]
    }
   ],
   "source": [
    "posterior_word_copy = create_posterior(2)\n",
    "\n",
    "def whole_corpus_gram(arg):\n",
    "    index = 0\n",
    "    count = 0\n",
    " \n",
    "    word_arr = []\n",
    "    \n",
    "    for word in corpus:\n",
    "        \n",
    "        prevToken = tokenize[word]\n",
    "        \n",
    "        word_arr.append(word)\n",
    "        \n",
    "        if (len(word_arr) == arg):\n",
    "            if index >= len(corpus) - 1:\n",
    "                break\n",
    "\n",
    "            newToken = 0\n",
    "            value = -1\n",
    "\n",
    "            predicted_word = pred_word(word_arr)\n",
    "\n",
    "            if predicted_word == corpus[index + 1]:\n",
    "                count += 1\n",
    "            word_arr.pop(0)\n",
    "\n",
    "            \n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "     \n",
    "    return count / (index - 1)\n",
    "\n",
    "print(whole_corpus_gram(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7869104984595939\n"
     ]
    }
   ],
   "source": [
    "posterior_word_copy = create_posterior(4)\n",
    "\n",
    "def whole_corpus_gram(arg):\n",
    "    index = 0\n",
    "    count = 0\n",
    " \n",
    "    word_arr = []\n",
    "    \n",
    "    for word in corpus:\n",
    "        \n",
    "        prevToken = tokenize[word]\n",
    "        \n",
    "        word_arr.append(word)\n",
    "        \n",
    "        if (len(word_arr) == arg):\n",
    "            if index >= len(corpus) - 1:\n",
    "                break\n",
    "\n",
    "            newToken = 0\n",
    "            value = -1\n",
    "\n",
    "            predicted_word = pred_word(word_arr)\n",
    "\n",
    "            if predicted_word == corpus[index + 1]:\n",
    "                count += 1\n",
    "            word_arr.pop(0)\n",
    "\n",
    "            \n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "     \n",
    "    return count / (index - 1)\n",
    "\n",
    "print(whole_corpus_gram(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908879058377439\n"
     ]
    }
   ],
   "source": [
    "posterior_word_copy = create_posterior(9)\n",
    "\n",
    "def whole_corpus_gram(arg):\n",
    "    index = 0\n",
    "    count = 0\n",
    " \n",
    "    word_arr = []\n",
    "    \n",
    "    for word in corpus:\n",
    "        \n",
    "        prevToken = tokenize[word]\n",
    "        \n",
    "        word_arr.append(word)\n",
    "        \n",
    "        if (len(word_arr) == arg):\n",
    "            if index >= len(corpus) - 1:\n",
    "                break\n",
    "\n",
    "            newToken = 0\n",
    "            value = -1\n",
    "\n",
    "            predicted_word = pred_word(word_arr)\n",
    "\n",
    "            if predicted_word == corpus[index + 1]:\n",
    "                count += 1\n",
    "            word_arr.pop(0)\n",
    "\n",
    "            \n",
    "        \n",
    "        index += 1\n",
    "        \n",
    "     \n",
    "    return count / (index - 1)\n",
    "\n",
    "print(whole_corpus_gram(9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with\n",
      "this\n",
      "a\n",
      "sudden\n",
      "alice\n",
      "but\n",
      "very\n",
      "glad\n",
      "they\n",
      "get\n",
      "to\n",
      "her\n",
      "eyes\n",
      "still\n",
      "it\n",
      "was\n",
      "very\n",
      "provoking\n",
      "to\n",
      "find\n",
      "that\n",
      "her\n",
      "neck\n",
      "as\n",
      "that\n"
     ]
    }
   ],
   "source": [
    "posterior_word_copy = create_posterior(3)\n",
    "\n",
    "random_arr = ['the', 'mad', 'hatter']\n",
    "\n",
    "for i in range(25):\n",
    "    pred_next_word = pred_word(random_arr)\n",
    "    \n",
    "    print(pred_next_word)\n",
    "    \n",
    "    random_arr.pop(0)\n",
    "    random_arr.append(pred_next_word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said\n",
      "alice\n",
      "replied\n",
      "rather\n",
      "shyly\n",
      "hardly\n",
      "know\n",
      "sir\n",
      "just\n",
      "at\n",
      "present\n",
      "least\n",
      "least\n",
      "least\n",
      "least\n",
      "notice\n",
      "of\n",
      "her\n",
      "going\n",
      "though\n",
      "as\n",
      "she\n",
      "was\n",
      "out\n",
      "exactly\n"
     ]
    }
   ],
   "source": [
    "posterior_word_copy = create_posterior(3)\n",
    "random_arr = ['the', 'mad', 'hatter']\n",
    "\n",
    "for i in range(25):\n",
    "    pred_next_word_array = pred_word_list_array(random_arr)\n",
    "    random_str = random.choices(wordlist, weights=pred_next_word_array, k=1)\n",
    "    \n",
    "    print(random_str[0])\n",
    "    \n",
    "    random_arr.pop(0)\n",
    "    random_arr.append(random_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
